{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b602e-87f6-42f0-b2f4-7b568a80f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image as Img\n",
    "from numpy import asarray, expand_dims\n",
    "from keras_facenet import FaceNet\n",
    "import numpy as np\n",
    "import pickle\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2e7b8-cc84-4b5f-9065-f8bd924d0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "HaarCascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n",
    "MyFaceNet = FaceNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c12068-56d5-4ba3-bd8c-1c4a382d9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liveness_model = tf.keras.models.load_model('liveness.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d18396-1203-4d0d-a046-9c6ed77672e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'UserImages/'  # Create a folder to store user images\n",
    "user_database = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74c246-c982-4742-90a6-d51f8af0f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_live(image):\n",
    "    # Resize the image to the input size expected by the liveness model\n",
    "    input_size = (32, 32)\n",
    "    image = cv2.resize(image, input_size)\n",
    "\n",
    "    # Preprocess the image (e.g., normalize pixel values)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Expand dimensions to match the input shape of the model\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Perform liveness detection prediction\n",
    "    prediction = liveness_model.predict(image)\n",
    "\n",
    "    # Determine if the face is live or not based on the prediction\n",
    "    if prediction[0][0] > 0.5:  # You may need to adjust the threshold\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def capture_and_train_user(username, num_images=20):\n",
    "    user_images = []  # Store user's face images\n",
    "    print(f\"Capturing images for user: {username}\")\n",
    "\n",
    "    # Create a directory for the user if it doesn't exist\n",
    "    user_dir = os.path.join(folder, username)\n",
    "    os.makedirs(user_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    image_count = 0\n",
    "\n",
    "    # Initialize the MTCNN face detection model\n",
    "    detector = MTCNN()\n",
    "\n",
    "    while image_count < num_images:\n",
    "        _, frame = cap.read()\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces using MTCNN\n",
    "        faces = detector.detect_faces(frame_rgb)\n",
    "\n",
    "        for result in faces:\n",
    "            x, y, w, h = result['box']\n",
    "\n",
    "            # Crop and resize the detected face region\n",
    "            face = frame_rgb[y:y + h, x:x + w]\n",
    "            face = Img.fromarray(face)\n",
    "            face = face.resize((160, 160))\n",
    "            face_array = asarray(face)\n",
    "            face_array = expand_dims(face_array, axis=0)\n",
    "\n",
    "            # Perform liveness detection\n",
    "            if is_live(face_array[0]):\n",
    "                user_images.append(face_array)\n",
    "\n",
    "                # Draw a rectangle around the detected face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                image_count += 1\n",
    "\n",
    "        cv2.imshow('Capture User Images', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Compute embeddings for user images\n",
    "    user_embeddings = [MyFaceNet.embeddings(image) for image in user_images]\n",
    "\n",
    "    # Store the user's embeddings in the database\n",
    "    user_database[username] = user_embeddings\n",
    "\n",
    "    # Save the user database to a file\n",
    "    with open(\"user_database.pkl\", \"wb\") as db_file:\n",
    "        pickle.dump(user_database, db_file)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"{num_images} images for user {username} captured and trained.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    username = input(\"Enter the username of the user: \")\n",
    "    capture_and_train_user(username, num_images=20)  # Adjust the number of images as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80caadf5-b36b-4f58-872a-963a985eb9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the user database\n",
    "with open(\"user_database.pkl\", \"rb\") as db_file:\n",
    "    user_database = pickle.load(db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6add3-7931-4e22-a24b-f2677b7473c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_live(image):\n",
    "    # Resize the image to the input size expected by the liveness model\n",
    "    input_size = (32, 32)\n",
    "    image = cv2.resize(image, input_size)\n",
    "\n",
    "    # Preprocess the image (e.g., normalize pixel values)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Expand dimensions to match the input shape of the model\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Perform liveness detection prediction\n",
    "    prediction = liveness_model.predict(image)\n",
    "\n",
    "    # Determine if the face is live or not based on the prediction\n",
    "    if prediction[0][0] > 0.5:  # You may need to adjust the threshold\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def recognize_user(face_embedding, face_image):\n",
    "    min_dist = 100  # Initialize minimum distance\n",
    "    recognized_user = None  # Initialize recognized user\n",
    "\n",
    "    for username, embeddings in user_database.items():\n",
    "        for user_embedding in embeddings:\n",
    "            dist = np.linalg.norm(user_embedding - face_embedding)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                recognized_user = username\n",
    "\n",
    "    if min_dist < 0.7:\n",
    "        # Perform liveness detection on the recognized face\n",
    "        if is_live(face_image):\n",
    "            return recognized_user\n",
    "        else:\n",
    "            return \"Live Person Detected (Not Authorized)\"\n",
    "    else:\n",
    "        return \"Unknown User\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = HaarCascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame_rgb[y:y + h, x:x + w]\n",
    "        face = Img.fromarray(face)\n",
    "        face = face.resize((160, 160))\n",
    "        face_array = asarray(face)\n",
    "        face_array = expand_dims(face_array, axis=0)\n",
    "\n",
    "        face_embedding = MyFaceNet.embeddings(face_array)\n",
    "\n",
    "        recognized_user = recognize_user(face_embedding, face_array[0])\n",
    "\n",
    "        # Display recognized user or \"Unknown User\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        font_thickness = 2\n",
    "        color = (0, 255, 0) if recognized_user != \"Unknown User\" else (0, 0, 255)\n",
    "        cv2.putText(frame, recognized_user, (x, y - 10), font, font_scale, color, font_thickness, cv2.LINE_AA)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
